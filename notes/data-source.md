# 数据来源

1. THULAC（清华大学）
   功能：高效中文分词 + 词性标注。
   词性体系：采用清华自定义的中文词性标签集（如 n 名词、v 动词、a 形容词等）。
   是否开源：是，提供 Python 和 C++ 版本。
   获取方式：需下载模型文件 Models_v1.zip，可从官方 GitHub 获取。
   适用场景：学术研究、工业应用均可
   。
   🔗 GitHub: https://github.com/thunlp/THULAC

2. LAC（百度）
   全称：Lexical Analysis of Chinese
   功能：分词、词性标注、命名实体识别（NER）一体化。
   词性标签：使用百度定义的细粒度词性体系（如 PER 人名、LOC 地点、TIME 时间等）。
   是否开源：是，支持 Python 和 C++
   。
   特点：工业级性能，准确率高。
   🔗 GitHub: https://github.com/baidu/lac

3. FoolNLTK
   特点：基于深度学习（BiLSTM），自称“最准的开源中文分词工具”。
   包含功能：分词、词性标注、实体识别
   。
   词性输出：兼容通用词性标签（如 n, v, ns 等）。
   适合人群：追求高准确率的开发者。
   🔗 GitHub: https://github.com/rockyzhengwu/FoolNLTK

4. HanLP（哈工大 & 多语言支持）
   功能全面：分词、词性标注、依存句法、语义角色标注等。
   词性体系：支持多种标准（如 CTB、PKU、863 等）。
   提供语料库：包含 CoNLL 格式的带词性标注的中文语料
   。
   可导出词典：通过 API 或模型反查，可构建按词性分类的词表。
   🔗 官网: https://hanlp.hankcs.com/

5. pkuseg（北京大学）
   支持词性标注，可加载预训练模型或自定义训练
   。
   提供细粒度领域模型（如新闻、网络、医学等）。
   📌 如何获取“按词性划分的词库”？
   这些工具本身不直接提供纯文本的“词性分类词表”（如 noun.txt, verb.txt），但你可以：

使用工具对大规模语料（如人民日报语料库）进行自动标注；
遍历输出结果，按词性标签分组收集词汇；
去重后生成自己的分类词库。
例如：用 THULAC 处理《人民日报》1998 年语料（已有带词性标注版本），或用 LAC 标注百科文本。

补充资源：公开带词性标注的语料库
人民日报 1998 语料库（PKU 标注）：广泛用于中文 NLP 训练，含分词和词性。
CTB（Chinese Treebank）：宾夕法尼亚大学发布，含精细词性与句法结构。
BosonNLP 语料：部分开放，含词性与情感标注。
